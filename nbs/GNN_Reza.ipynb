{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "moNbiPPDrxHV"
   },
   "source": [
    "# **Install libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ok9vuEA-yCkr",
    "outputId": "b0ed9514-e8ac-4aec-e4e5-356e95fb57a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting torch_geometric\n",
      "  Downloading torch_geometric-2.3.1.tar.gz (661 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.65.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.22.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.10.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.27.1)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.0.9)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.1.0)\n",
      "Building wheels for collected packages: torch_geometric\n",
      "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for torch_geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910459 sha256=0e9f38ffd919cd153db9895224362c00327003f69405092ba2f0a5e4179ee091\n",
      "  Stored in directory: /root/.cache/pip/wheels/ac/dc/30/e2874821ff308ee67dcd7a66dbde912411e19e35a1addda028\n",
      "Successfully built torch_geometric\n",
      "Installing collected packages: torch_geometric\n",
      "Successfully installed torch_geometric-2.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch_geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTKOshJ7b4x7"
   },
   "source": [
    "# **Parser**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGvRrCYjqT9r"
   },
   "source": [
    "Please change the directory to the proper one that you've saved the dataset in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cfJ3Eb7IcxI-",
    "outputId": "3492dc72-f0f0-4e70-d514-10ddc5721752"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/gdrive/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/gdrive/MyDrive/University/HLS/GNN/\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive/')\n",
    "directory = '/content/gdrive/MyDrive/University/HLS/GNN/'\n",
    "file_list = ['syrk.c.021t.ssa.c', '2mm.c.021t.ssa.c', 'jacobi_2d.c.021t.ssa.c', 'adi.c.021t.ssa.c', 'fdtd_2d.c.021t.ssa.c',\n",
    "             'Tsyrk.c.021t.ssa.c', 'T2mm.c.021t.ssa.c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SRA change\n",
    "directory = 'data/' \n",
    "file_list = ['syrk.c.021t.ssa.c', '2mm.c.021t.ssa.c', 'jacobi_2d.c.021t.ssa.c', 'adi.c.021t.ssa.c', 'fdtd_2d.c.021t.ssa.c',\n",
    "             'Tsyrk.c.021t.ssa.c', 'T2mm.c.021t.ssa.c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "FWnQB42_b8Ug"
   },
   "outputs": [],
   "source": [
    "def check_finish_teavers(visted_vertex, BB_adjacent_mat):\n",
    "    for vertex in visted_vertex:\n",
    "        if visted_vertex[vertex] < len(BB_adjacent_mat[vertex]):\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def adj_mat_fun(filename):\n",
    "  #The file name should be dumped using -fdump-tree-ssa-gimple. It's the 021 tree.\n",
    "  lines    = []\n",
    "\n",
    "  with open(filename, \"r\") as file:\n",
    "      for line in file:\n",
    "          lines.append(line.strip())\n",
    "\n",
    "  #################################################################################\n",
    "  #Create the list of using variables.\n",
    "\n",
    "  function_name    = 0  #Here I just enumerate\n",
    "  initial_part     = 0\n",
    "  var_dict_temp    = {} #It's local for any function and has the temperory vars\n",
    "  var_dict_static  = {} #It's local for any function and has the args and defined vars\n",
    "  var_all          = {} #Key's are corresponded to the funcation name and values are the var_dict_temp of each function\n",
    "  fun_def          = 0  #check for __GIMPLE to see if we are at the beginning of a function\n",
    "\n",
    "  for line in lines:\n",
    "      words = line.split()  \n",
    "      \n",
    "      if fun_def: #since we know the immidiate line after the __GIMPLE is the argument lines of the function\n",
    "          fun_def = 0\n",
    "          args = line.split(sep=',') #first split it based on ','\n",
    "          for arg in args:\n",
    "              arg_list = arg.split()\n",
    "              if '(' not in arg: #check if it is the first arguman and has the function name and other stuff in it\n",
    "                  if ')' not in arg: #check if it is the last arguman and has the last ')'\n",
    "                      var_dict_static[arg_list[-1]] = arg[1:-(len(arg_list[-1])+1)]\n",
    "                  else:\n",
    "                      var_dict_static[arg_list[-1][0:-1]] = arg[1:-(len(arg_list[-1])+1)]\n",
    "              else:\n",
    "                  par_ind = arg.find('(')\n",
    "                  if ')' not in arg:\n",
    "                      var_dict_static[arg_list[-1]] = arg[par_ind+1:-(len(arg_list[-1])+1)]\n",
    "                  else:\n",
    "                      if '(' not in arg_list[-1]:\n",
    "                          var_dict_static[arg_list[-1][0:-1]] = arg[par_ind+1:-(len(arg_list[-1])+1)]\n",
    "\n",
    "      for word in words:\n",
    "          if '__GIMPLE' in word: #check for the __GIMPLE that is a distinctive annotation for the beginning of the function\n",
    "              fun_def                 = 1\n",
    "          \n",
    "          if '{' in word: #check to see if we are in the function scope.\n",
    "              initial_part            = 1 #since in the SSA all the initializations are at the beginning\n",
    "          if '}' in word: #check to see if we are out of the function scope.\n",
    "              var_all_local           = {} #to have temp and static vars in dict\n",
    "              var_all_local['static'] = var_dict_static\n",
    "              var_all_local['temp']   = var_dict_temp\n",
    "              var_all[function_name]  = var_all_local #updating var_all for values in the parsed function\n",
    "              var_dict_temp           = {} #clear the local var_dict_temp\n",
    "              var_dict_static         = {} #clear the local var_dict_static\n",
    "              function_name          += 1 #increase the index of function\n",
    "          if '__BB' in word:\n",
    "              initial_part            = 0 #check the end of initialization\n",
    "      \n",
    "      if initial_part and len(words)>1:\n",
    "          if words[-1][0] == '_': #having '_' at the beginning indicates it's temperoray and made by the compiler\n",
    "              var_dict_temp[words[-1][0:-1]]   = line[0:-(len(words[-1])+1)]\n",
    "          else:\n",
    "              var_dict_static[words[-1][0:-1]] = line[0:-(len(words[-1])+1)]\n",
    "          \n",
    "  #################################################################################\n",
    "  #Create the adjacent matrix for BBs\n",
    "  fun_def                  = 0  #check for __GIMPLE to see if we are at the beginning of a function\n",
    "  function_scope           = 0  #to check if we are inside a scope\n",
    "  function_name            = 0  #Here I just enumerate\n",
    "  BB_name                  = 0  #to have the name of current BB. It's only valid if BB_scope=1\n",
    "  BB_ind                   = -1 #to store a numeric value for BBs.\n",
    "  BB_scope                 = 0  #to check if we are inside a scope\n",
    "  BB_first_line            = 0  #to store the BBs name and info\n",
    "  BB_local_dict            = {} #it has the 0-indexed vertex as key and BBs name in val for each function\n",
    "  BB_global_dict           = {} #it has the 0-indexed vertex as key and BBs name in val for all functoins\n",
    "  BB_rev_local_dict        = {} #it has the 0-indexed vertex as val and BBs name in key for each function\n",
    "  BB_rev_global_dict       = {} #it has the 0-indexed vertex as val and BBs name in key for all functoins\n",
    "  BB_adjacent_mat          = [] #it's for all the BBs inside a function\n",
    "  BB_local_adj             = [] #it's local to each BB in a function\n",
    "  BB_adjacent_mat_global   = {} #it's for all of the funcitons\n",
    "  BB_livness               = [] #it's for all the BBs inside a function\n",
    "  BB_livness_local         = set() #it's local to each BB in a function\n",
    "  BB_livness_gloabal       = {} #it's for all of the funcitons\n",
    "  BB_livness_start         = {} #it's for all the BBs inside a function\n",
    "  BB_livness_gloabal_start = {} #it's for all of the funcitons\n",
    "  BB_all_val               = [] #have all the variable use in a function\n",
    "  BB_all_val_global        = {} #have entire variables\n",
    "  for line in lines:\n",
    "      words = line.split()\n",
    "\n",
    "      if fun_def: #since we know the immidiate line after the __GIMPLE is the argument lines of the function\n",
    "          fun_def = 0\n",
    "\n",
    "      if '__GIMPLE' in line: #check for the __GIMPLE that is a distinctive annotation for the beginning of the function\n",
    "          fun_def                                 = 1\n",
    "          \n",
    "      if '{' in line: #check to see if we are in the function scope.\n",
    "          function_scope                          = 1 #check if we are inside of the function scope \n",
    "      if '}' in line: #check to see if we are out of the function scope.\n",
    "          function_scope                          = 0   \n",
    "          BB_global_dict[function_name]           = BB_local_dict\n",
    "          BB_local_dict                           = {}\n",
    "          BB_rev_global_dict[function_name]       = BB_rev_local_dict\n",
    "          BB_rev_local_dict                       = {}\n",
    "          BB_adjacent_mat.append(BB_local_adj)\n",
    "          BB_adjacent_mat_global[function_name]   = BB_adjacent_mat\n",
    "          BB_local_adj                            = []\n",
    "          BB_adjacent_mat                         = []\n",
    "          BB_livness.append(BB_livness_local)\n",
    "          BB_livness_gloabal[function_name]       = BB_livness\n",
    "          BB_livness_gloabal_start[function_name] = BB_livness_start\n",
    "          BB_livness_local                        = set()\n",
    "          BB_livness                              = []\n",
    "          BB_livness_start                        = {}\n",
    "          BB_ind                                  = -1\n",
    "          function_name                          += 1 #increase the index of function\n",
    "\n",
    "      if line[0:4] == '__BB':\n",
    "          BB_scope                        = 1 #check that we are inside a BB\n",
    "          BB_first_line                   = 1\n",
    "      \n",
    "      if BB_first_line: #store BBs data\n",
    "          if BB_ind>=0: #not for the first one\n",
    "              BB_adjacent_mat.append(BB_local_adj)\n",
    "              BB_livness.append(BB_livness_local)\n",
    "          BB_local_adj                    = [] \n",
    "          BB_livness_local                = set()\n",
    "          BB_livness_local_start          = set()\n",
    "          BB_first_line                   = 0 #reset the first line flag for BBs\n",
    "          start                           = line.find('(')+1\n",
    "          if line.find(',') > 0: #check if there is other info\n",
    "              end                         = line.find(',')\n",
    "          else:  \n",
    "              end                         = line.find(')') \n",
    "          BB_name                         = line[start: end]\n",
    "          BB_ind                         += 1\n",
    "          BB_local_dict[BB_ind]           = int(BB_name) #update the local dict for BBs' name\n",
    "          BB_rev_local_dict[int(BB_name)] = BB_ind\n",
    "      \n",
    "      if BB_scope:\n",
    "          if ' =' in line: #Check if there is an assignment. This can be consider as the start of the livenss analysis since the tree is in SSA form\n",
    "              if '__MEM' not in words[0]:\n",
    "                  BB_livness_local.add(words[0])\n",
    "                  BB_livness_start[words[0]] = BB_ind\n",
    "              \n",
    "              for word in words:\n",
    "                  var  = ''\n",
    "                  vars = set()\n",
    "                  if '_' in word and '__' not in word:\n",
    "                      for char in word:\n",
    "                          asci = ord(char)\n",
    "                          if (asci<=90 and asci >=65) or (asci<=57 and asci>=48) or (asci<=122 and asci>=97) or char =='_':\n",
    "                              var += char                \n",
    "                          else:\n",
    "                              if len(var)>0:\n",
    "                                  vars.add(var)\n",
    "                                  var = ''\n",
    "                      vars.add(var)\n",
    "                      for var in vars:\n",
    "                          if '_' in var:\n",
    "                              BB_livness_local.add(var)\n",
    "\n",
    "          if 'goto' in line:\n",
    "              start = line.find('B')+2\n",
    "              BB_local_adj.append(int(line[start:-1]))\n",
    "          if 'return' in line:\n",
    "              BB_local_adj.append(-1)\n",
    "\n",
    "  #################################################################################\n",
    "  #Traversing the tree to check the liveness\n",
    "\n",
    "  live_global    = BB_livness_gloabal #To store liveness for each BBs of all function \n",
    "  live_local     = set() #To store liveness for each BBs of each function \n",
    "  live           = [] #to store liveness for each function\n",
    "  visited        = [] #To store visited edge of all functions\n",
    "  visited_times  = [] #To store number of visited edge of all functions\n",
    "  function_name  = -1\n",
    "\n",
    "  for BB_adjacent_mat_key in BB_adjacent_mat_global:\n",
    "      BB_adjacent_mat = BB_adjacent_mat_global[BB_adjacent_mat_key]\n",
    "      function_name += 1\n",
    "      visited_times  = []\n",
    "      for vertex in BB_adjacent_mat: #set all vertex zero visited\n",
    "          visited_times.append(0)    \n",
    "      while check_finish_teavers(visited_times, BB_adjacent_mat): #Continue until all paths have been visited\n",
    "          next = 2\n",
    "          visited = []\n",
    "\n",
    "          while next!=-1:#Continue until the last BB\n",
    "              visited.append(next)\n",
    "              BB_ind   = BB_rev_global_dict[function_name][next]\n",
    "              len_next = len(BB_adjacent_mat[BB_ind])\n",
    "              if visited_times[BB_ind] < len_next:\n",
    "                  next = BB_adjacent_mat[BB_ind][visited_times[BB_ind]]\n",
    "                  visited_times[BB_ind] += 1\n",
    "              else:\n",
    "                  next = BB_adjacent_mat[BB_ind][visited_times[BB_ind]-1]\n",
    "\n",
    "          for var in BB_livness_gloabal_start[function_name]:#Update the liveness of the BB in the path\n",
    "              BB_ind = BB_livness_gloabal_start[function_name][var]\n",
    "              BB     = BB_global_dict[function_name][BB_ind]\n",
    "              start  = visited.index(BB)\n",
    "              last   = start\n",
    "              for i in range(start+1,len(visited)):\n",
    "                  BB_ind_visited = BB_rev_global_dict[function_name][visited[i]]\n",
    "                  if var in BB_livness_gloabal[function_name][BB_ind_visited]:\n",
    "                      last = i\n",
    "              for i in range(start+1, last+1):\n",
    "                  BB_ind_visited = BB_rev_global_dict[function_name][visited[i]]\n",
    "                  live_global[function_name][BB_ind_visited].add(var)\n",
    "\n",
    "\n",
    "  ##################################################################################            \n",
    "  \n",
    "\n",
    "  BB_ind = 0\n",
    "  adj_list = []\n",
    "  for BB_local_adj in BB_adjacent_mat_global[0]:\n",
    "      for adj in BB_local_adj:\n",
    "          if adj > 0:\n",
    "              adj_list.append([BB_global_dict[0][BB_ind]-2, adj-2])\n",
    "      BB_ind += 1\n",
    "\n",
    "  BB_ind = 0\n",
    "  live_list = []\n",
    "  for BB_livness in BB_livness_gloabal[0]:\n",
    "      live_list.append([len(BB_livness)/81])\n",
    "      BB_ind += 1\n",
    "  BB_ind = 0\n",
    "\n",
    "  for BB_livness in live_global[0]:\n",
    "      live_list[BB_ind].append(len(BB_livness)/88)\n",
    "      BB_ind += 1\n",
    "  \n",
    "  return [live_list, adj_list]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptrXJud1o7c9"
   },
   "source": [
    "The bellow part is not complete yet and it's not connected to the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lkwCGSSrp1Pn",
    "outputId": "428d406a-1475-489d-cbd4-499aee44b2a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have not dumped the corresponded AST and the program cannot do it either\n",
      "please type the function name:\n",
      "dsa\n",
      "BBs:\n",
      "{}\n",
      "Loops:\n",
      "[]\n",
      "args:\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# Import library\n",
    "#import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "#Functions\n",
    "\n",
    "#Check if a word (seprated by space) is a variable\n",
    "def check_var(var, word):\n",
    "    asciis = [x for x in range(60,91)]\n",
    "    asciis.extend([x for x in range(97,123)])\n",
    "    asciis.extend([x for x in range(48,58)])\n",
    "    asciis.append(95)\n",
    "    if var in word:\n",
    "        if (' ' + var + '[' in word) or (' ' + var + '(' in word) or (' ' + var + ' ' in word):\n",
    "            return True\n",
    "        for x in asciis:\n",
    "            if var + chr(x) in word:\n",
    "                return False\n",
    "        for x in asciis:\n",
    "            if chr(x) + var in word:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "\n",
    "# Reading the AST file\n",
    "lines = []\n",
    "try:\n",
    "    f = open(AST_filename, 'r')\n",
    "    for line in f:\n",
    "        lines.append(line)\n",
    "    f.close()\n",
    "except:\n",
    "    print('You have not dumped the corresponded AST and the program cannot do it either')\n",
    "\n",
    "\n",
    "# Find nodes that have jumps/branches\n",
    "current_bb     = 0\n",
    "branch_bb      = 0\n",
    "main_scope     = 0\n",
    "last_bb        = 0\n",
    "branching_node = []\n",
    "loops          = []\n",
    "loops_initials = []\n",
    "\n",
    "param_nums     = 0\n",
    "\n",
    "args = dict()\n",
    "\n",
    "#All possible variables to be declared in the scope\n",
    "cpp_var = ['char', 'char16_t', 'char32_t', 'wchar_t',\n",
    "           'signed char', 'signed short int', 'short int', 'signed short', 'short', 'signed int', 'int', 'signed long int', 'signed long', 'long int', 'long', 'signed long long int', 'signed long long', 'long long int', 'long long',\n",
    "           'unsigned char', 'unsigned short int', 'unsigned short', 'unsigned int', 'unsigned', 'unsigned long int', 'unsigned long', 'unsigned long long int', 'unsigned long long',\n",
    "           'float', 'double', 'long double',\n",
    "           'bool',\n",
    "           'void',\n",
    "           'decltype(nullptr)',\n",
    "           'size_t']\n",
    "\n",
    "#Asking for the name of specific function to make the json file for\n",
    "print('please type the function name:')\n",
    "function_name = input()\n",
    "\n",
    "#Going through each line of the AST to extract the features\n",
    "for line in lines:\n",
    "\n",
    "    #make a list of words splitted by space out of each line\n",
    "    line_words_list = line.split()\n",
    "\n",
    "    #checking if the requried function has started  \n",
    "    if function_name in line_words_list and ';;' not in line_words_list:\n",
    "        main_scope    = 1\n",
    "        inside_args   = 0\n",
    "        val_flag      = 0\n",
    "        previous_word = ''\n",
    "\n",
    "        #Find the args of the function\n",
    "        for word in line_words_list:\n",
    "            if not inside_args and '(' in word:\n",
    "                inside_args   = 1\n",
    "                previous_word = word[1:]\n",
    "                val_flag      = 1\n",
    "\n",
    "            elif inside_args and val_flag:\n",
    "                if ')' in word:\n",
    "                    inside_args         = 0\n",
    "                    args[word[0:len(word)-1]] = previous_word\n",
    "                    val_flag            = 0\n",
    "                elif '*' in word:\n",
    "                    previous_word      += '*'\n",
    "                else:\n",
    "                    args[word[0:len(word)-1]] = previous_word\n",
    "                    val_flag            = 0\n",
    "            \n",
    "            elif inside_args and not val_flag:\n",
    "                previous_word = word\n",
    "                val_flag      = 1\n",
    "\n",
    "    #Checking for the end of the function       \n",
    "    if '{' in line_words_list:\n",
    "        param_nums += 1\n",
    "    if '}' in line_words_list:\n",
    "        param_nums -= 1\n",
    "        if param_nums == 0:\n",
    "            main_scope = 0\n",
    "\n",
    "    #Finding the requried features inside the main scope of the function\n",
    "    if main_scope:\n",
    "        if 'goto' in line_words_list:\n",
    "            flag_bb_found = 0\n",
    "            for word in line_words_list:\n",
    "                if word == '<bb':\n",
    "                    flag_bb_found = 1\n",
    "                elif flag_bb_found:\n",
    "                    branch_bb = int(word.split(sep='>')[0])\n",
    "                    flag_bb_found = 0\n",
    "            branching_node.append([current_bb, branch_bb])\n",
    "\n",
    "            if(current_bb > branch_bb):\n",
    "                loops.append([x for x in range(branch_bb, current_bb+1)])\n",
    "                loops_initials.append(current_bb)\n",
    "\n",
    "        else:\n",
    "            flag_bb_found = 0\n",
    "            for word in line_words_list:\n",
    "                if word == '<bb':\n",
    "                    flag_bb_found = 1\n",
    "                elif flag_bb_found:\n",
    "                    current_bb = int(word.split(sep='>')[0])\n",
    "                    flag_bb_found = 0\n",
    "                    if current_bb > last_bb:\n",
    "                        last_bb = current_bb\n",
    "\n",
    "\n",
    "# Define type\n",
    "bb_edge_type = dict()\n",
    "for i in range(2,last_bb+1):\n",
    "    occurence_num = 0\n",
    "    for j in branching_node:\n",
    "        if j[0] == i:\n",
    "            occurence_num += 1\n",
    "    if i in loops_initials:\n",
    "        bb_edge_type[i] = 'Start of loop'\n",
    "    elif occurence_num >= 1:\n",
    "        bb_edge_type[i] = 'Multi-branch'\n",
    "   # elif occurence_num == 1:\n",
    "   #     bb_edge_type[i] = 'End of loop'\n",
    "    else:\n",
    "        bb_edge_type[i] = 'Simple'\n",
    "\n",
    "#Printing the extracting features\n",
    "print(\"BBs:\")\n",
    "print(bb_edge_type)\n",
    "\n",
    "print(\"Loops:\")\n",
    "print(loops)\n",
    "\n",
    "print('args:')\n",
    "print(args)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9iFmcRUwO-Wn"
   },
   "outputs": [],
   "source": [
    "graphs = []\n",
    "for f in file_list:\n",
    "  filename = directory + f\n",
    "  g = adj_mat_fun(filename)\n",
    "  graphs.append(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0.012345679012345678, 0.011363636363636364],\n",
       "  [0.037037037037037035, 0.03409090909090909],\n",
       "  [0.24691358024691357, 0.22727272727272727],\n",
       "  [0.06172839506172839, 0.056818181818181816],\n",
       "  [0.037037037037037035, 0.03409090909090909],\n",
       "  [0.06172839506172839, 0.056818181818181816],\n",
       "  [0.4567901234567901, 0.42045454545454547],\n",
       "  [0.08641975308641975, 0.07954545454545454],\n",
       "  [0.06172839506172839, 0.056818181818181816],\n",
       "  [0.06172839506172839, 0.056818181818181816],\n",
       "  [0.037037037037037035, 0.03409090909090909],\n",
       "  [0.037037037037037035, 0.03409090909090909],\n",
       "  [0.0, 0.0]],\n",
       " [[0, 11],\n",
       "  [1, 3],\n",
       "  [2, 3],\n",
       "  [3, 2],\n",
       "  [3, 4],\n",
       "  [4, 9],\n",
       "  [5, 7],\n",
       "  [6, 7],\n",
       "  [7, 6],\n",
       "  [7, 8],\n",
       "  [8, 9],\n",
       "  [9, 5],\n",
       "  [9, 10],\n",
       "  [10, 11],\n",
       "  [11, 1],\n",
       "  [11, 12]]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SRA\n",
    "len(graphs)\n",
    "graphs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OD4o8Iyc8gai"
   },
   "source": [
    "# **Building Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Uxm3SBTRzUWX"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def graph_to_data(G, label=[0, 0, 0, 0, 1]):\n",
    "    edge_index = torch.tensor(list(G[1])).t().contiguous()\n",
    "    x = torch.tensor(G[0])  \n",
    "    y = torch.tensor([label])  \n",
    "    train_mask = torch.tensor([True])  \n",
    "    \n",
    "    return Data(x=x, edge_index=edge_index, y=y, train_mask=train_mask)\n",
    "\n",
    "data = [graph_to_data(graphs[0], [1, 0, 0, 0, 0]), \n",
    "        graph_to_data(graphs[1], [0, 1, 0, 0, 0]),\n",
    "        graph_to_data(graphs[2], [0, 0, 1, 0, 0]),\n",
    "        graph_to_data(graphs[3], [0, 0, 0, 1, 0]),\n",
    "        graph_to_data(graphs[4], [0, 0, 0, 0, 1]),\n",
    "        graph_to_data(graphs[5], [1, 0, 0, 0, 0]),\n",
    "        graph_to_data(graphs[6], [0, 1, 0, 0, 0])]\n",
    "\n",
    "train_data = data[:5]\n",
    "test_data  = data[5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QSMabYAxdOl"
   },
   "source": [
    "# **GNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: tensor([[0.0123, 0.0114],\n",
      "        [0.0370, 0.0341],\n",
      "        [0.2469, 0.2273],\n",
      "        [0.0617, 0.0568],\n",
      "        [0.0370, 0.0341],\n",
      "        [0.0617, 0.0568],\n",
      "        [0.4568, 0.4205],\n",
      "        [0.0864, 0.0795],\n",
      "        [0.0617, 0.0568],\n",
      "        [0.0617, 0.0568],\n",
      "        [0.0370, 0.0341],\n",
      "        [0.0370, 0.0341],\n",
      "        [0.0000, 0.0000]])\n",
      "\n",
      "labels: tensor([[1, 0, 0, 0, 0]])\n",
      "\n",
      "edge index\" tensor([[ 0,  1,  2,  3,  3,  4,  5,  6,  7,  7,  8,  9,  9, 10, 11, 11],\n",
      "        [11,  3,  3,  2,  4,  9,  7,  7,  6,  8,  9,  5, 10, 11,  1, 12]])\n"
     ]
    }
   ],
   "source": [
    "#SRA\n",
    "print(f'features: {data[0].x}')\n",
    "print(f'\\nlabels: {data[0].y}')\n",
    "print(f'\\nedge index\" {data[0].edge_index}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "BBwan_3gxgj7"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "'''\n",
    "SRA comment:\n",
    "Assuming that the labels are mutually exclusive (i.e. an input can't have more than one \"1\" in the label array),\n",
    "we want the model's output to be a set of K probabilities (K = #distinct number of unique labels) that add to 1.\n",
    "\n",
    "Softmax(x) = exp(x_i) / Sum_j(exp(x_j))\n",
    "\n",
    "converts an array of numbers x, to an array of probabilities. The exp is to make numbers non-negative and the division\n",
    "by the sum is to normalize the sum of probs to 1.\n",
    "'''\n",
    "\n",
    "# Define the GNN architecture\n",
    "class GNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels=2, out_channels=16)\n",
    "        self.conv2 = GCNConv(in_channels=16, out_channels=32)\n",
    "        self.fc = nn.Linear(32, 5)  \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax()\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu() #SRA: adding another relu before fc layer (alt conv and relu)\n",
    "        x = self.fc(x)\n",
    "        #x = F.dropout(x, p=0.5, training=self.training)\n",
    "        \n",
    "        #x = self.sigmoid(x) #SRA: changing to softmax - next line\n",
    "        #x = self.softmax(x) #SRA: changing to softmax\n",
    "        #see comment below on \"logits\" and why an output activation is not needed\n",
    "        \n",
    "        x = torch.mean(x, dim=0, keepdim=True) #SRA comment: mean over embeddings for full graph\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBYFqrYK8mRB"
   },
   "source": [
    "# **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fms6vCXl8lMO",
    "outputId": "b34c42a8-159b-4bd9-c7f8-5c7d04f1965c",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 1.6553784370422364, Test Avg. Accuracy: 0.0\n",
      "Epoch: 51, Train Loss: 1.0107943795621395, Test Avg. Accuracy: 0.0\n",
      "Epoch: 101, Train Loss: 0.6816273195669055, Test Avg. Accuracy: 0.5\n",
      "Epoch: 151, Train Loss: 0.4271356512093917, Test Avg. Accuracy: 0.5\n",
      "Epoch: 201, Train Loss: 0.2683346415986307, Test Avg. Accuracy: 0.0\n",
      "Epoch: 251, Train Loss: 0.1674283431319054, Test Avg. Accuracy: 0.0\n",
      "Epoch: 301, Train Loss: 0.10167338917090092, Test Avg. Accuracy: 0.0\n",
      "Epoch: 351, Train Loss: 0.06263789805670968, Test Avg. Accuracy: 0.0\n",
      "Epoch: 401, Train Loss: 0.03996627299056854, Test Avg. Accuracy: 0.0\n",
      "Epoch: 451, Train Loss: 0.02664802250365028, Test Avg. Accuracy: 0.0\n",
      "Epoch: 501, Train Loss: 0.018433041265961948, Test Avg. Accuracy: 0.0\n",
      "Epoch: 551, Train Loss: 0.013253079903370235, Test Avg. Accuracy: 0.0\n",
      "Epoch: 601, Train Loss: 0.009858745141536928, Test Avg. Accuracy: 0.0\n",
      "Epoch: 651, Train Loss: 0.007473975302855251, Test Avg. Accuracy: 0.0\n",
      "Epoch: 701, Train Loss: 0.005780022449107492, Test Avg. Accuracy: 0.0\n",
      "Epoch: 751, Train Loss: 0.004533609573627473, Test Avg. Accuracy: 0.0\n",
      "Epoch: 801, Train Loss: 0.0035995020985865266, Test Avg. Accuracy: 0.0\n",
      "Epoch: 851, Train Loss: 0.0028979882425119287, Test Avg. Accuracy: 0.0\n",
      "Epoch: 901, Train Loss: 0.002348645357051282, Test Avg. Accuracy: 0.0\n",
      "Epoch: 951, Train Loss: 0.001918133782237419, Test Avg. Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "\n",
    "model = GNNModel()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "\n",
    "#criterion = nn.BCEWithLogitsLoss() #SRA: commenting BCE and replacing with CrossEntropyLoss below\n",
    "criterion = nn.CrossEntropyLoss() #SRA: please see comment below\n",
    "\n",
    "'''\n",
    "Change 1: BCE vs CrossEntropy\n",
    "BCEWithLogitsLoss is used when one is doing binary classification i.e. the output is 0/1 (two classes)\n",
    "\n",
    "More precisely, the labels are assumed to be 0/1 (or [1,0] and [0,1]) and the \n",
    "predictions are two probabilities [p1, p2] with p1 + p2 == 1\n",
    "\n",
    "If the label is [1,0]: BCE loss = -log(p1)\n",
    "If the label is [0,1]: BCE loss = -log(p2)\n",
    "\n",
    "If the appropriate pj == 1, then the model predicted the output with prob 1 and the loss==0\n",
    "If the appropriate pj == 0, then the model predicted the output with prob 0 and the loss==\\infty\n",
    "\n",
    "CrossEntropyLoss is a generalization of BCE loss to more than two output categories. The loss is still\n",
    "-log(p_k) where class k is the correct label.\n",
    "\n",
    "CrossEntropyLoss doc: https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "\n",
    "Change 2: Output activation vs no output activation\n",
    "For both BCEWithLogits and CrossEntropyLoss, the predicted is expected to be the \"logits\" i.e.\n",
    "the network's output before applying softmax (the raw x_j passes to the softmax)\n",
    "\n",
    "This is done for numerical reasons so we are not exponentiating and then taking logs of the inputs.\n",
    "\n",
    "Practically, this means we need to remove the softmax from the forward() function in the GNN\n",
    "\n",
    "'''\n",
    "\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.01) #SRA: changing to AdamW - not crucial\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2) #SRA: making lr much smaller for now\n",
    "\n",
    "# Define the training loop\n",
    "#SRA: todo: we'll add batching here later\n",
    "def train(model, data, optimizer, criterion):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = criterion(output, data.y.float())  # Apply sigmoid activation and compare with target labels\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(data)\n",
    "        predicted_labels = output > 0.5  # Threshold the output for binary predictions\n",
    "        accuracy = (predicted_labels == data.y).all(dim=1).float().mean()\n",
    "    return accuracy.item()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "print_freq = 50 #adding printing every print_freq epochs\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    for data in train_data:\n",
    "        train_loss += train(model, data, optimizer, criterion)\n",
    "    train_loss /= len(train_data)\n",
    "    test_scores = []\n",
    "    for data in test_data:\n",
    "        test_scores.append(evaluate(model, data))\n",
    "    test_avg_score = sum(test_scores) / len(test_scores)\n",
    "    if epoch % print_freq == 0: #SRA\n",
    "        print(f\"Epoch: {epoch+1}, Train Loss: {train_loss}, Test Avg. Accuracy: {test_avg_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "moNbiPPDrxHV",
    "KTKOshJ7b4x7",
    "OD4o8Iyc8gai",
    "4QSMabYAxdOl",
    "EBYFqrYK8mRB"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
